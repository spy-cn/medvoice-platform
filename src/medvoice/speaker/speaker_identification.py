import logging
import os
import sys
import warnings
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Tuple

import numpy as np
import static_ffmpeg
from funasr import AutoModel
from pydub import AudioSegment
from sklearn.metrics.pairwise import cosine_similarity

from src.medvoice.utils import audio_utils
from src.medvoice.utils.logger_utils import setup_logger

static_ffmpeg.add_paths()

warnings.filterwarnings('ignore')

logger = setup_logger('SpeakerIdentification', level=logging.DEBUG)

project_root = Path(__file__).resolve().parents[3]  # è°ƒæ•´å±‚çº§
sys.path.append(str(project_root))


@dataclass
class SpeakerSegment:
    """è¯´è¯äººéŸ³é¢‘ç‰‡æ®µæ•°æ®ç±»"""
    start_time: float  # å¼€å§‹æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    end_time: float  # ç»“æŸæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    text: str  # è¯†åˆ«æ–‡æœ¬
    spk_code: str  # è¯´è¯äººç¼–ç 
    spk_name: str  # è¯´è¯äººå§“å
    speaker_id: str  # è¯´è¯äººID
    audio_path: str  # éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    similarity: float  # ç›¸ä¼¼åº¦å¾—åˆ†


class SpeakerIdentification:
    def __init__(self):
        self.asr_model = None
        self.sv_model = None
        self.speaker_profiles = {}  # å­˜å‚¨å·²çŸ¥è¯´è¯äººçš„å£°çº¹ç‰¹å¾
        self.speaker_names = {}  # è¯´è¯äººIDåˆ°å§“åçš„æ˜ å°„
        self.speaker_counter = 1  # è¯´è¯äººè®¡æ•°å™¨
        self.similarity_threshold = 0.7  # ç›¸ä¼¼åº¦é˜ˆå€¼
        self.temp_dir = r"E:\code_project\medvoice-recognition-platform\data\audio"

    def init_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹"""
        try:
            # åˆå§‹åŒ–ASRæ¨¡å‹ï¼ˆåŒ…å«è¯´è¯äººåˆ†ç¦»ï¼‰
            self.asr_model = AutoModel(
                model="paraformer-zh",
                vad_model="fsmn-vad",
                punc_model="ct-punc",
                spk_model="cam++",
                disable_update=True
            )
            logger.info("âœ… ASRæ¨¡å‹åŠ è½½æˆåŠŸ")

            # åˆå§‹åŒ–å£°çº¹è¯†åˆ«æ¨¡å‹
            self.sv_model = AutoModel(
                model="cam++",
                disable_update=True
            )
            logger.info("âœ… å£°çº¹è¯†åˆ«æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def _extract_voiceprint_embedding(self, audio_path: str) -> Optional[np.ndarray]:
        """æå–å£°çº¹åµŒå…¥å‘é‡"""
        try:
            if not os.path.exists(audio_path):
                logger.error(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
                return None

            # éŸ³é¢‘è´¨é‡æ£€æµ‹å’Œå¢å¼º
            audio_segment = AudioSegment.from_file(audio_path)
            audio_quality = audio_utils.assess_speech_quality(audio_segment)

            if audio_quality < 0.5:
                logger.debug(f"éŸ³é¢‘è´¨é‡è¾ƒä½({audio_quality:.3f})ï¼Œè¿›è¡Œå¢å¼º: {audio_path}")
                enhanced_audio = self._enhance_audio_quality(audio_segment, target_duration=2000)
                enhanced_path = audio_path.replace('.wav', '_enhanced.wav')
                enhanced_audio.export(enhanced_path, format="wav")
                audio_path = enhanced_path  # ä½¿ç”¨å¢å¼ºåçš„éŸ³é¢‘

            # æå–å£°çº¹ç‰¹å¾
            result = self.sv_model.generate(input=audio_path)

            # æ¸…ç†ä¸´æ—¶å¢å¼ºæ–‡ä»¶
            if '_enhanced' in audio_path and os.path.exists(audio_path):
                os.remove(audio_path)

            return self._process_embedding_result(result)

        except Exception as e:
            logger.error(f"æå–å£°çº¹ç‰¹å¾å¤±è´¥ {audio_path}: {e}")
            return None

    def _enhance_audio_quality(self, audio: AudioSegment, target_duration: int, max_attempts: int = 3) -> AudioSegment:
        """éŸ³é¢‘è´¨é‡å¢å¼º"""
        enhanced_audio = audio_utils.repeat_audio_pydub_exact(audio, target_duration)
        quality_score = 0.0
        for attempt in range(1, max_attempts + 1):
            quality_score = audio_utils.assess_speech_quality(enhanced_audio)
            if quality_score >= 0.5:
                logger.debug(f"âœ… éŸ³é¢‘å¢å¼ºæˆåŠŸ (ç¬¬{attempt}æ¬¡å°è¯•, è´¨é‡: {quality_score:.3f})")
                return enhanced_audio
            elif attempt < max_attempts:
                logger.debug(f"ğŸ”„ ç»§ç»­éŸ³é¢‘å¢å¼º (ç¬¬{attempt}æ¬¡å°è¯•, è´¨é‡: {quality_score:.7f})")
                enhanced_audio = audio_utils.repeat_audio_pydub_exact(enhanced_audio, target_duration * attempt)

        logger.warning(f"âš ï¸ éŸ³é¢‘å¢å¼ºæœªè¾¾ç†æƒ³è´¨é‡ (æœ€ç»ˆè´¨é‡: {quality_score:.7f})")
        return enhanced_audio

    def _process_embedding_result(self, result):

        if result and isinstance(result, list) and len(result) > 0:
            embedding = None

            if 'spk_embedding' in result[0]:
                embedding_tensor = result[0]['spk_embedding']
                embedding = embedding_tensor.cpu().numpy() if hasattr(embedding_tensor, 'cpu') else np.array(
                    embedding_tensor)

            if embedding is not None:
                # æ ‡å‡†åŒ–ç»´åº¦
                if len(embedding.shape) == 1:
                    embedding = embedding.reshape(1, -1)
                elif len(embedding.shape) == 2 and embedding.shape[0] > 1:
                    embedding = embedding[0].reshape(1, -1)

                # L2å½’ä¸€åŒ–
                embedding = embedding / np.linalg.norm(embedding)
            return embedding
        return None

    def collect_speaker_voiceprints(self,
                                    speaker_name: str, audio_paths: List[str], min_audio_count: int = 3,
                                    quality_threshold: float = 0.4) -> Optional[str]:
        """
        æ”¶é›†ç”¨æˆ·å£°çº¹ä¿¡æ¯
        :param speaker_name: è¯´è¯äººå§“å
        :param audio_paths: éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        :param min_audio_count: æœ€å°‘éœ€è¦çš„åˆæ ¼éŸ³é¢‘æ•°é‡
        :param quality_threshold: éŸ³é¢‘è´¨é‡é˜ˆå€¼
        :return: è¯´è¯äººID æˆ–Nonde
        """
        logger.info(f"å¼€å§‹ä¸º{speaker_name}æ”¶é›†å£°çº¹ä¿¡æ¯...")
        logger.info(f"å¾…å¤„ç†éŸ³é¢‘æ•°é‡ï¼š{len(audio_paths)}")
        collect_embeddings = []
        quality_scores = []

        for i, audio_path in enumerate(audio_paths):
            if not os.path.exists(audio_path):
                logger.error(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼š{audio_path}")
                continue
            try:
                # 1ã€å…ˆæ£€æŸ¥éŸ³é¢‘è´¨é‡
                need_audio_segment = AudioSegment.from_file(audio_path)
                audio_quality_score = audio_utils.assess_speech_quality(need_audio_segment)
                logger.debug(f"éŸ³é¢‘è´¨é‡é¢‘åˆ†ä¸ºï¼š{audio_quality_score}")
                # å¦‚æœéŸ³é¢‘è´¨é‡è¯„åˆ†ä¸è¾¾æ ‡ è¿›è¡Œå¢å¼º
                if audio_quality_score < quality_threshold:
                    audio = AudioSegment.from_file(audio_path)
                    repeat_audio = audio_utils.repeat_audio_pydub_exact(audio, 2000)
                    enhancement_attempt = 0
                    max_enhancement_attempts = 2
                    while enhancement_attempt <= max_enhancement_attempts:
                        quality_score = audio_utils.assess_speech_quality(repeat_audio)
                        if quality_score > quality_threshold:
                            break
                        else:
                            if enhancement_attempt < max_enhancement_attempts:
                                # å†æ¬¡å¢å¼º
                                enhancement_attempt += 1
                                logger.debug(
                                    f"ğŸ”„ å°è¯•ç¬¬ {enhancement_attempt} æ¬¡é‡æ–°å¢å¼ºä½è´¨é‡éŸ³é¢‘: {os.path.basename(audio_path)}")
                            else:
                                logger.debug(
                                    f"âš ï¸ ä½è´¨é‡éŸ³é¢‘(å·²è¾¾æœ€å¤§å¢å¼ºæ¬¡æ•°): {max_enhancement_attempts} (è´¨é‡: {quality_score})")
                                break
                else:
                    quality_scores.append(audio_quality_score)
                # 2ã€æå–å£°çº¹ä¿¡æ¯
                embedding = self._extract_voiceprint_embedding(audio_path)
                collect_embeddings.append(embedding)
            except Exception as e:
                logger.error(e)

        if len(collect_embeddings) < min_audio_count:
            logger.error(f"æœ‰æ•ˆå£°çº¹æ•°é‡ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {min_audio_count} ä¸ªï¼Œå½“å‰ {len(collect_embeddings)} ä¸ª")
            return None
        # 3ã€è´¨é‡åŠ æƒå¹³å‡èåˆ
        logger.debug("æ­£åœ¨è¿›è¡Œå£°çº¹ç‰¹å¾èåˆ...")
        combined_embedding = self._fuse_voiceprints(collect_embeddings, quality_scores)
        # 4ã€æ³¨å†Œè¯´è¯äºº
        spk_code = self._register_or_update_speaker(speaker_name, combined_embedding)
        logger.info(f"æˆåŠŸä¸º '{speaker_name}' æ³¨å†Œå£°çº¹ï¼ŒCODE: {spk_code}")
        return spk_code

    def process_audio_with_spk_diarization(self, audio_path: str, hotword: str = None) -> List[SpeakerSegment]:
        """
        å¤„ç†éŸ³é¢‘å¹¶è¿›è¡Œè¯´è¯äººåˆ†ç¦»å’Œè¯†åˆ«
        :param audio_path: è¦å¤„ç†çš„éŸ³é¢‘è·¯å¾„
        :param hotword: çƒ­è¯
        :return:
        """
        if not self.asr_model:
            logger.error("ASRæ¨¡å‹æœªåˆå§‹åŒ–")
            return []

        if not os.path.exists(audio_path):
            logger.error(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
            return []

        logger.info(f"å¼€å§‹å¤„ç†éŸ³é¢‘: {audio_path}")
        try:
            # è¯´è¯äººåˆ†ç¦»
            res = self.asr_model.generate(
                input=audio_path,
                batch_size_s=300,
                hotword=hotword
            )
            if not res:
                logger.error("æ²¡æœ‰è¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³")
                return []
            # å¤„ç†æ¯ä¸€ä¸ªè¯­éŸ³ç‰‡æ®µ
            speaker_segments = []
            audio = AudioSegment.from_file(audio_path)
            for i, segment in enumerate(res):
                sentence_info_list = segment.get('sentence_info', [])
                for j, sentence_info in enumerate(sentence_info_list):
                    segment_result = self._process_single_segment(
                        sentence_info, audio, i, j
                    )
                    if segment_result:
                        speaker_segments.append(segment_result)
            # è¯†åˆ«è¯´è¯äºº
            identified_segments = self._identify_speakers_in_segments(speaker_segments)

            self._print_recognition_results(identified_segments)
            return identified_segments
        except Exception as e:
            logger.error(f"å¤„ç†éŸ³é¢‘å¤±è´¥ï¼š{e}")
            return []

        pass

    def _fuse_voiceprints(self, embeddings: List[np.ndarray], quality_scores: List[float]) -> np.ndarray:
        """
        èåˆå¤šä¸ªå£°çº¹ç‰¹å¾
        :param embeddings:
        :param quality_scores:
        :return:
        """
        # å½’ä¸€åŒ–è´¨é‡æƒé‡
        weights = np.array(quality_scores) / sum(quality_scores)

        # è®¡ç®—åŠ æƒå¹³å‡
        combined_embedding = np.average(embeddings, axis=0, weights=weights)

        # L2å½’ä¸€åŒ–
        combined_embedding = combined_embedding / np.linalg.norm(combined_embedding)

        return combined_embedding

    def _register_or_update_speaker(self, speaker_name: str, embedding: np.ndarray) -> str:
        """
        æ³¨å†Œæˆ–è€…æ›´æ–°è¯´è¯äºº
        :param speaker_name:  è¯´è¯äººå§“å
        :param embedding:  å£°çº¹å‘é‡
        :return:
        """
        # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨æ­¤è¯´è¯äºº
        existing_code = None
        for spk_code, name in self.speaker_names.items():
            if name == speaker_name:
                existing_code = spk_code
                break
        if existing_code:
            # æ›´æ–°ç°æœ‰è¯´è¯äººï¼ˆæŒ‡æ•°å¹³æ»‘æ›´æ–°ï¼‰
            old_embedding = self.speaker_profiles[existing_code]
            updated_embedding = 0.7 * embedding + 0.3 * old_embedding
            updated_embedding = updated_embedding / np.linalg.norm(updated_embedding)

            self.speaker_profiles[existing_code] = updated_embedding
            logger.info(f"å·²æ›´æ–°è¯´è¯äºº: {speaker_name} (CODE: {existing_code})")
            return existing_code
        else:
            # æ–°æ³¨å†Œ
            spk_code = f"spk_{self.speaker_counter:03d}"
            self.speaker_counter += 1
            self.speaker_profiles[spk_code] = embedding
            self.speaker_names[spk_code] = speaker_name
            logger.info(f"å·²æ³¨å†Œæ–°è¯´è¯äºº: {speaker_name} (CODE: {spk_code})")
            return spk_code

    def _process_single_segment(self, sentence_info: dict, audio: AudioSegment,
                                segment_idx: int, sentence_idx: int) -> Optional[SpeakerSegment]:
        """
        å¤„ç†å•ä¸ªè¯­éŸ³ç‰‡æ®µ
        :param sentence_info:
        :param audio:
        :param segment_idx:
        :param sentence_idx:
        :return:
        """
        try:
            segments_dir = os.path.join(self.temp_dir, "segments")
            os.makedirs(segments_dir, exist_ok=True)

            text = sentence_info.get('text', '')
            start_time = sentence_info.get('start', 0)
            end_time = sentence_info.get('end', 0)
            spk_id = sentence_info.get('spk', 'æœªçŸ¥')
            logger.debug(f"è¯´è¯äººï¼š{spk_id}")
            # æå–éŸ³é¢‘ç‰‡æ®µ
            segment_audio = audio[start_time:end_time]
            # ä¿å­˜ç‰‡æ®µ
            segment_filename = f"segment_{segment_idx}_{sentence_idx}_{start_time}_{end_time}.wav"
            segment_path = os.path.join(self.temp_dir, "segments", segment_filename)
            logger.debug(f"ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼š{segment_path}")
            segment_audio.export(segment_path, format="wav")
            return SpeakerSegment(
                start_time=start_time,
                end_time=end_time,
                text=text,
                speaker_id=spk_id,
                spk_code="",
                spk_name=spk_id,
                audio_path=segment_path,
                similarity=0.0
            )
        except Exception as e:
            logger.error(f"å¤„ç†è¯­éŸ³ç‰‡æ®µå¤±è´¥:{e}")
            return None

    def _identify_speakers_in_segments(self, speaker_segments: List[SpeakerSegment]) -> List[SpeakerSegment]:
        """
        è¯†åˆ«ç‰‡æ®µä¸­çš„è¯´è¯äºº
        :param speaker_segments:
        :return:
        """
        identified_segments = []
        spk_map = {}
        max_similarity_map = {}
        for segment in speaker_segments:

            query_embedding = self._extract_voiceprint_embedding(segment.audio_path)
            if query_embedding is None:
                logger.warning(f"æœªè¯†åˆ«åˆ°è¯´è¯äººID:{segment.audio_path}")
                segment.speaker_id = "unknown"
                identified_segments.append(segment)
                continue
            duration = segment.end_time - segment.start_time
            dynamic_threshold = self._get_dynamic_threshold(duration)
            best_match_spk_code, best_score = self._match_against_voiceprint_library(query_embedding, dynamic_threshold)
            if best_match_spk_code:
                spk_map[segment.speaker_id] = best_match_spk_code
                segment.spk_code = best_match_spk_code
                segment.similarity = best_score
                # æ›´æ–°spk_mapï¼Œåªä¿ç•™ç›¸ä¼¼åº¦æœ€å¤§çš„æ˜ å°„
                if segment.speaker_id and segment.speaker_id != "unknown":
                    current_max_similarity = max_similarity_map.get(segment.speaker_id, -1)
                    if best_score > current_max_similarity:
                        spk_map[segment.speaker_id] = best_match_spk_code
                        max_similarity_map[segment.speaker_id] = best_score
                        logger.debug(f"æ›´æ–°æ˜ å°„: {segment.speaker_id} -> {best_match_spk_code}, ç›¸ä¼¼åº¦: {best_score}")
            else:
                segment.spk_code = spk_map.get(segment.speaker_id, "unknown")
                segment.similarity = best_score
            identified_segments.append(segment)
        print(f"======================:{spk_map}")
        return identified_segments

    def _get_dynamic_threshold(self, duration: float) -> float:
        """æ ¹æ®éŸ³é¢‘æ—¶é•¿åŠ¨æ€è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼"""
        base_threshold = self.similarity_threshold

        if duration < 1000:  # å°‘äº1ç§’
            return max(0.5, base_threshold - 0.2)
        elif duration < 2000:  # 1-2ç§’
            return max(0.6, base_threshold - 0.1)
        else:  # 2ç§’ä»¥ä¸Š
            return base_threshold

    def _match_against_voiceprint_library(self, query_embedding: np.ndarray,
                                          threshold: float) -> Tuple[Optional[str], float]:
        """ä¸å£°çº¹åº“è¿›è¡ŒåŒ¹é…"""
        best_match_spk_code = None
        best_score = 0.0

        for spk_code, profile_embedding in self.speaker_profiles.items():
            try:
                # ç¡®ä¿ç»´åº¦ä¸€è‡´
                if query_embedding.shape[1] != profile_embedding.shape[1]:
                    continue

                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = cosine_similarity(query_embedding, profile_embedding)[0][0]
                logger.debug(f"è¯´è¯äººcodeï¼š{spk_code},ç›¸ä¼¼åº¦ï¼š{similarity}")
                if similarity > best_score and similarity >= threshold:
                    best_score = similarity
                    best_match_spk_code = spk_code

            except Exception as e:
                logger.error(f"ä¸è¯´è¯äºº {spk_code} æ¯”å¯¹å¤±è´¥: {e}")
                continue

        return best_match_spk_code, best_score

    def _print_recognition_results(self, segments: List[SpeakerSegment]):
        """æ‰“å°è¯†åˆ«ç»“æœ"""
        logger.debug("\n" + "=" * 60)
        logger.debug("è¯´è¯äººè¯†åˆ«ç»“æœ")
        logger.debug("=" * 60)

        for segment in segments:
            speaker_name = self.speaker_names.get(segment.spk_code, "æœªçŸ¥è¯´è¯äºº")

            if segment.spk_code != "unknown":
                logger.debug(f"âœ… è¯†åˆ«åˆ°: {speaker_name} (ç›¸ä¼¼åº¦: {segment.similarity:.3f})")
            else:
                logger.debug(f"âŒ æœªè¯†åˆ« (æœ€é«˜ç›¸ä¼¼åº¦: {segment.similarity:.3f})")
            logger.debug(f"è¯´è¯äººIDï¼š: {segment.speaker_id}")
            logger.debug(f"è¯´è¯äººCODEï¼š: {segment.spk_code}")
            logger.debug(f"æ—¶é—´: {segment.start_time / 1000:.2f}s - {segment.end_time / 1000:.2f}s")
            logger.debug(f"å†…å®¹: {segment.text}")
            logger.debug(f"éŸ³é¢‘è·¯å¾„: {segment.audio_path}")
            logger.debug("-" * 40)


if __name__ == "__main__":
    spkIdent = SpeakerIdentification()
    spkIdent.init_models()
    speakers_data = {
        "å¶é—®è€å©†": [
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\å¶é—®è€å©†1.wav',
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\å¶é—®è€å©†2.wav',
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\å¶é—®è€å©†3.wav',
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\å¶é—®è€å©†4.wav',
        ],
        # "å¶é—®": [
        #     r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\å¶é—®1.wav',
        #     r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\å¶é—®2.wav',
        #     r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\å¶é—®3.wav',
        # ],
        "å¶é—®è®²è§£è€…": [
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\å¶é—®è®²è§£è€…1.wav',
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\å¶é—®è®²è§£è€…2.wav',
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\å¶é—®è®²è§£è€…3.wav',
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\å¶é—®è®²è§£è€…4.wav',
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\å¶é—®è®²è§£è€…5.wav',
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\å¶é—®è®²è§£è€…6.wav',
        ],
        "è·¯äºº": [
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\è·¯äºº1.wav',
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\è·¯äºº2.wav',
        ],
        "é‡‘å±±çˆª": [
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\é‡‘å±±çˆª0.wav',
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\é‡‘å±±çˆª1.wav',
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\é‡‘å±±çˆª2.wav',
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\é‡‘å±±çˆª3.wav',
            r'E:\code_project\medvoice-recognition-platform\data\audio\voiceprint_lib\é‡‘å±±çˆª.wav',
        ]
    }
    for speaker_name, audio_paths in speakers_data.items():
        spk_code = spkIdent.collect_speaker_voiceprints(speaker_name, audio_paths, min_audio_count=2,
                                                        quality_threshold=0.4)
        if spk_code:
            logger.info(f"âœ… æˆåŠŸæ³¨å†Œ: {speaker_name} -> {spk_code}")
        else:
            logger.error(f"âŒ æ³¨å†Œå¤±è´¥: {speaker_name}")
    target_audio = r"E:\code_project\medvoice-recognition-platform\data\audio\origin_audio\å¶é—®.wav"
    if os.path.exists(target_audio):
        segments = spkIdent.process_audio_with_spk_diarization(target_audio)
        logger.debug(f"å¤„ç†å®Œæˆï¼Œå…±è¯†åˆ« {len(segments)} ä¸ªè¯­éŸ³ç‰‡æ®µ")
    else:
        logger.debug(f"ç›®æ ‡éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {target_audio}")
